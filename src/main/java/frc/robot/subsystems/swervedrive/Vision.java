package frc.robot.subsystems.swervedrive;

import edu.wpi.first.apriltag.AprilTagFieldLayout;
import edu.wpi.first.apriltag.AprilTagFields;
import edu.wpi.first.math.geometry.Pose2d;
import edu.wpi.first.math.geometry.Pose3d;
import edu.wpi.first.math.geometry.Transform2d;
import edu.wpi.first.wpilibj.Timer;
import edu.wpi.first.wpilibj.smartdashboard.Field2d;
import frc.robot.Cameras;
import frc.robot.Robot;
import java.awt.Desktop;
import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;
import java.util.Optional;
import java.util.function.Supplier;
import org.photonvision.EstimatedRobotPose;
import org.photonvision.PhotonUtils;
import org.photonvision.simulation.VisionSystemSim;
import org.photonvision.targeting.PhotonPipelineResult;
import org.photonvision.targeting.PhotonTrackedTarget;
import swervelib.SwerveDrive;

/**
 * Example PhotonVision class to aid in the pursuit of accurate odometry. Taken from
 * https://gitlab.com/ironclad_code/ironclad-2024/-/blob/master/src/main/java/frc/robot/vision/Vision.java?ref_type=heads
 */
public class Vision {

  /** April Tag Field Layout of the year. */
  public static final AprilTagFieldLayout fieldLayout =
      AprilTagFieldLayout.loadField(AprilTagFields.kDefaultField);

  /** Ambiguity defined as a value between (0,1). Used in {@link Vision#filterPose}. */
  private final double maximumAmbiguity = 0.25;

  /** Photon Vision Simulation */
  public VisionSystemSim visionSim;

  /** Count of times that the odom thinks we're more than 10meters away from the april tag. */
  private double longDistangePoseEstimationCount = 0;

  /** Current pose from the pose estimator using wheel odometry. */
  private Supplier<Pose2d> currentPose;

  /** Field from {@link swervelib.SwerveDrive#field} */
  private Field2d field2d;

  List<PoseObservation> poseObservations = new LinkedList<>();

  private final List<Short> tagIds = new ArrayList<>();

  // Target lock tracking (for ReadyToShoot composite)
  private static final int LOCK_THRESHOLD_FRAMES = 5;
  private int consecutiveTargetFrames = 0;
  private double lastTargetTimestamp = 0;
  private boolean lastHasTarget = false;

  /**
   * Constructor for the Vision class.
   *
   * @param currentPose Current pose supplier, should reference {@link SwerveDrive#getPose()}
   * @param field Current field, should be {@link SwerveDrive#field}
   */
  public Vision(Supplier<Pose2d> currentPose, Field2d field) {
    this.currentPose = currentPose;
    this.field2d = field;

    if (Robot.isSimulation()) {
      visionSim = new VisionSystemSim("Vision");
      visionSim.addAprilTags(fieldLayout);

      for (Cameras c : Cameras.values()) {
        c.addToVisionSim(visionSim);
      }

      openSimCameraViews();
    }
  }

  /**
   * Calculates a target pose relative to an AprilTag on the field.
   *
   * @param aprilTag The ID of the AprilTag.
   * @param robotOffset The offset {@link Transform2d} of the robot to apply to the pose for the
   *     robot to position itself correctly.
   * @return The target pose of the AprilTag.
   */
  public static Pose2d getAprilTagPose(int aprilTag, Transform2d robotOffset) {
    Optional<Pose3d> aprilTagPose3d = fieldLayout.getTagPose(aprilTag);
    if (aprilTagPose3d.isPresent()) {
      return aprilTagPose3d.get().toPose2d().transformBy(robotOffset);
    } else {
      throw new RuntimeException(
          "Cannot get AprilTag " + aprilTag + " from field " + fieldLayout.toString());
    }
  }

  /**
   * Update the pose estimation inside of {@link SwerveDrive} with all of the given poses.
   *
   * @param swerveDrive {@link SwerveDrive} instance.
   */
  public void updatePoseEstimation(SwerveDrive swerveDrive) {
    if (swerveDrive.getSimulationDriveTrainPose().isPresent()) {
      /*
       * In the maple-sim, odometry is simulated using encoder values, accounting for
       * factors like skidding and drifting.
       * As a result, the odometry may not always be 100% accurate.
       * However, the vision system should be able to provide a reasonably accurate
       * pose estimation, even when odometry is incorrect.
       * (This is why teams implement vision system to correct odometry.)
       * Therefore, we must ensure that the actual robot pose is provided in the
       * simulator when updating the vision simulation during the simulation.
       */
      visionSim.update(swerveDrive.getSimulationDriveTrainPose().get());
    }
    for (Cameras camera : Cameras.values()) {
      Optional<EstimatedRobotPose> poseEst = getEstimatedGlobalPose(camera);
      if (poseEst.isPresent()) {
        var pose = poseEst.get();
        swerveDrive.addVisionMeasurement(
            pose.estimatedPose.toPose2d(), pose.timestampSeconds, camera.curStdDevs);
      }
    }
  }

  /**
   * Generates the estimated robot pose. Returns empty if:
   *
   * <ul>
   *   <li>No Pose Estimates could be generated
   *   <li>The generated pose estimate was considered not accurate
   * </ul>
   *
   * @return an {@link EstimatedRobotPose} with an estimated pose, timestamp, and targets used to
   *     create the estimate
   */
  public Optional<EstimatedRobotPose> getEstimatedGlobalPose(Cameras camera) {
    Optional<EstimatedRobotPose> poseEst = camera.getEstimatedGlobalPose();
    if (Robot.isSimulation()) {
      Field2d debugField = visionSim.getDebugField();
      // Uncomment to enable outputting of vision targets in sim.
      poseEst.ifPresentOrElse(
          est -> debugField.getObject("VisionEstimation").setPose(est.estimatedPose.toPose2d()),
          () -> {
            debugField.getObject("VisionEstimation").setPoses();
          });
    }
    return poseEst;
  }

  /**
   * Filter pose via the ambiguity and find best estimate between all of the camera's throwing out
   * distances more than 10m for a short amount of time.
   *
   * @param pose Estimated robot pose.
   * @return Could be empty if there isn't a good reading.
   */

  /**
   * Get distance of the robot from the AprilTag pose.
   *
   * @param id AprilTag ID
   * @return Distance
   */
  public double getDistanceFromAprilTag(int id) {
    Optional<Pose3d> tag = fieldLayout.getTagPose(id);
    return tag.map(pose3d -> PhotonUtils.getDistanceToPose(currentPose.get(), pose3d.toPose2d()))
        .orElse(-1.0);
  }

  /**
   * Get tracked target from a camera of AprilTagID
   *
   * @param id AprilTag ID
   * @param camera Camera to check.
   * @return Tracked target.
   */
  public PhotonTrackedTarget getTargetFromId(int id, Cameras camera) {
    PhotonTrackedTarget target = null;
    for (PhotonPipelineResult result : camera.resultsList) {
      if (result.hasTargets()) {
        for (PhotonTrackedTarget i : result.getTargets()) {
          if (i.getFiducialId() == id) {
            return i;
          }
        }
      }
    }
    return target;
  }

  /**
   * Vision simulation.
   *
   * @return Vision Simulation
   */
  public VisionSystemSim getVisionSim() {
    return visionSim;
  }

  /**
   * Open up the photon vision camera streams on the localhost, assumes running photon vision on
   * localhost.
   */
  private void openSimCameraViews() {
    if (Desktop.isDesktopSupported() && Desktop.getDesktop().isSupported(Desktop.Action.BROWSE)) {
      // try
      // {
      // Desktop.getDesktop().browse(new URI("http://localhost:1182/"));
      // Desktop.getDesktop().browse(new URI("http://localhost:1184/"));
      // Desktop.getDesktop().browse(new URI("http://localhost:1186/"));
      // } catch (IOException | URISyntaxException e)
      // {
      // e.printStackTrace();
      // }
    }
  }

  public Cameras getbestCamera(int id) {
    PhotonTrackedTarget bestTarget = null;
    Cameras bestCamEnum = null;

    for (Cameras c : Cameras.values()) {
      Optional<PhotonPipelineResult> optionalResult = c.getLatestResult();

      if (optionalResult.isPresent() && optionalResult.get().hasTargets()) {
        for (var target : optionalResult.get().getTargets()) {
          if (target.getFiducialId() == id) {
            if (bestTarget == null || target.getArea() > bestTarget.getArea()) {
              bestTarget = target;
              bestCamEnum = c;
            }
          }
        }
      }
    }
    return bestCamEnum; // can be null if no camera sees the target
  }

  /** Update the {@link Field2d} to include tracked targets/ */
  public void updateVisionField() {

    List<PhotonTrackedTarget> targets = new ArrayList<PhotonTrackedTarget>();
    for (Cameras c : Cameras.values()) {
      if (!c.resultsList.isEmpty()) {
        PhotonPipelineResult latest = c.resultsList.get(0);
        if (latest.hasTargets()) {
          targets.addAll(latest.targets);
        }
      }
    }

    List<Pose2d> poses = new ArrayList<>();
    for (PhotonTrackedTarget target : targets) {
      if (fieldLayout.getTagPose(target.getFiducialId()).isPresent()) {
        Pose2d targetPose = fieldLayout.getTagPose(target.getFiducialId()).get().toPose2d();
        poses.add(targetPose);
      }
    }

    field2d.getObject("tracked targets").setPoses(poses);
  }

  public static record PoseObservation(
      double timestamp,
      Pose3d pose,
      double ambiguity,
      int tagCount,
      double averageTagDistance,
      PhotonPipelineResult bestResult) {}

  /** Check if any camera currently sees a target. */
  public boolean hasTarget() {
    for (Cameras c : Cameras.values()) {
      Optional<PhotonPipelineResult> result = c.getLatestResult();
      if (result.isPresent() && result.get().hasTargets()) {
        return true;
      }
    }
    return false;
  }

  /**
   * Get the fiducial ID of the best visible target (largest area).
   *
   * @return Tag ID, or -1 if no target visible
   */
  public int getBestTargetId() {
    PhotonTrackedTarget bestTarget = null;
    for (Cameras c : Cameras.values()) {
      Optional<PhotonPipelineResult> result = c.getLatestResult();
      if (result.isPresent() && result.get().hasTargets()) {
        for (PhotonTrackedTarget target : result.get().getTargets()) {
          if (bestTarget == null || target.getArea() > bestTarget.getArea()) {
            bestTarget = target;
          }
        }
      }
    }
    return (bestTarget != null) ? bestTarget.getFiducialId() : -1;
  }

  /**
   * Check if vision is locked on target (stable tracking for ReadyToShoot). Requires consecutive
   * frames with targets to confirm lock.
   */
  public boolean isLockedOnTarget() {
    return consecutiveTargetFrames >= LOCK_THRESHOLD_FRAMES;
  }

  /**
   * Update target lock tracking and log vision telemetry. Call this in periodic or after
   * updatePoseEstimation.
   */
  public void updateTargetLock() {
    double now = Timer.getFPGATimestamp();
    boolean currentHasTarget = hasTarget();

    if (currentHasTarget) {
      consecutiveTargetFrames++;
      lastTargetTimestamp = now;
    } else {
      consecutiveTargetFrames = 0;
    }
    lastHasTarget = currentHasTarget;
  }

  /**
   * Get pipeline latency from the best available result with targets.
   *
   * @return Latency in ms, or -1 if no result available
   */
  public double getBestTargetLatencyMs() {
    for (Cameras c : Cameras.values()) {
      Optional<PhotonPipelineResult> result = c.getLatestResult();
      if (result.isPresent() && result.get().hasTargets()) {
        return result.get().metadata.getLatencyMillis();
      }
    }
    return -1;
  }

  /**
   * Check if a specific camera is connected.
   *
   * @param cam Camera to check
   * @return true if connected
   */
  public boolean isCameraConnected(Cameras cam) {
    return cam != null && cam.camera.isConnected();
  }
}
